{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2b65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6276491",
   "metadata": {},
   "source": [
    "### Combine old and new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_old.csv')\n",
    "df.head()\n",
    "# df = old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72667f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = df[\"text_original\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b975c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = [text.replace (\"\\n\", \"\") for text in news]\n",
    "clean_text = [\" \".join(text.split()) for text in news] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min = df[[\"filename\", \"text_original\", \"text_clean\",\"date\", \"year\", \"month\", \"outlet\", \"country\", \"south\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data_new.csv')\n",
    "df2.drop(labels = ['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df2.head()\n",
    "# df2 = new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ce6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df_min, df2])\n",
    "df3 = df3.reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a08c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95861b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76168be0",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71dd8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdeb705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text_original</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>outlet</th>\n",
       "      <th>country</th>\n",
       "      <th>south</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Act now'_ APRA issues climate change advice.rtf</td>\n",
       "      <td>\\nNovember 27, 2021 Saturday\\nAustralian3 Edit...</td>\n",
       "      <td>November 27, 2021 Saturday Australian3 Edition...</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>The Australian</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Australia must keep 95pc coal in ground'.rtf</td>\n",
       "      <td>\\nSeptember 9, 2021 Thursday\\nAustralian Editi...</td>\n",
       "      <td>September 9, 2021 Thursday Australian Edition ...</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>The Australian</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Ban new coalmines, gas and oil fields to hit ...</td>\n",
       "      <td>\\nMay 19, 2021 Wednesday\\nAustralian Edition\\n...</td>\n",
       "      <td>May 19, 2021 Wednesday Australian Edition Copy...</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>The Australian</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Big picture' Bill trips again on the details,...</td>\n",
       "      <td>\\nMay 1, 2019 Wednesday\\nAustralian Edition\\n\\...</td>\n",
       "      <td>May 1, 2019 Wednesday Australian Edition Copyr...</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>The Australian</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Business is trying to regain the trust of the...</td>\n",
       "      <td>\\nJuly 2, 2019 Tuesday\\nAustralian Edition\\n\\n...</td>\n",
       "      <td>July 2, 2019 Tuesday Australian Edition Copyri...</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>The Australian</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0   'Act now'_ APRA issues climate change advice.rtf   \n",
       "1      'Australia must keep 95pc coal in ground'.rtf   \n",
       "2  'Ban new coalmines, gas and oil fields to hit ...   \n",
       "3  'Big picture' Bill trips again on the details,...   \n",
       "4  'Business is trying to regain the trust of the...   \n",
       "\n",
       "                                       text_original  \\\n",
       "0  \\nNovember 27, 2021 Saturday\\nAustralian3 Edit...   \n",
       "1  \\nSeptember 9, 2021 Thursday\\nAustralian Editi...   \n",
       "2  \\nMay 19, 2021 Wednesday\\nAustralian Edition\\n...   \n",
       "3  \\nMay 1, 2019 Wednesday\\nAustralian Edition\\n\\...   \n",
       "4  \\nJuly 2, 2019 Tuesday\\nAustralian Edition\\n\\n...   \n",
       "\n",
       "                                          text_clean        date  year  month  \\\n",
       "0  November 27, 2021 Saturday Australian3 Edition...  2021-11-01  2021     11   \n",
       "1  September 9, 2021 Thursday Australian Edition ...  2021-09-01  2021      9   \n",
       "2  May 19, 2021 Wednesday Australian Edition Copy...  2021-05-01  2021      5   \n",
       "3  May 1, 2019 Wednesday Australian Edition Copyr...  2019-05-01  2019      5   \n",
       "4  July 2, 2019 Tuesday Australian Edition Copyri...  2019-07-01  2019      7   \n",
       "\n",
       "           outlet    country  south  \n",
       "0  The Australian  Australia      0  \n",
       "1  The Australian  Australia      0  \n",
       "2  The Australian  Australia      0  \n",
       "3  The Australian  Australia      0  \n",
       "4  The Australian  Australia      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(labels = ['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17801d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124535"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c28b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c92a7",
   "metadata": {},
   "source": [
    "Derived from: https://saturncloud.io/blog/algorithm-to-detect-similar-documents-in-python-script/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675c5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df[\"text_clean\"].to_list()\n",
    "\n",
    "threshold = 0.9\n",
    "vectorizer = TfidfVectorizer()\n",
    "similar_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c38dff0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents)):\n\u001b[1;32m      4\u001b[0m     text2 \u001b[38;5;241m=\u001b[39m documents[j][\u001b[38;5;241m200\u001b[39m:\u001b[38;5;241m3200\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(vectors)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m threshold:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2137\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n\u001b[0;32m-> 2137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1710\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1710\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1714\u001b[0m         X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:845\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[1;32m    844\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m--> 845\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:561\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    556\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt check \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m sparse matrix for nan or inf.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m spmatrix\u001b[38;5;241m.\u001b[39mformat,\n\u001b[1;32m    558\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    559\u001b[0m         )\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 122\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    text1 = documents[i][200:3200]\n",
    "    for j in range(i+1, len(documents)):\n",
    "        text2 = documents[j][200:3200]\n",
    "        vectors = vectorizer.fit_transform([text1, text2])\n",
    "        similarity = cosine_similarity(vectors)\n",
    "        if similarity[1][0] > threshold:\n",
    "            print (i, j, similarity[1][0])\n",
    "            similar_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels = ['level_0'], axis = 1, inplace = True)\n",
    "df = df.drop(similar_index).reset_index()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c8816",
   "metadata": {},
   "source": [
    "**Enable multiple cores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c128749",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df[\"text_clean\"].to_list()\n",
    "\n",
    "threshold = 0.9\n",
    "vectorizer = TfidfVectorizer()\n",
    "similar_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(i, text1):\n",
    "    for j in range(i+1, len(documents)):\n",
    "        text2 = documents[j][200:3200]\n",
    "        vectors = vectorizer.fit_transform([text1, text2])\n",
    "        similarity = cosine_similarity(vectors)\n",
    "        if similarity[1][0] > threshold:\n",
    "            return i, j, similarity[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocessing_func(i):\n",
    "    text1 = documents[i][200:3200]\n",
    "    y = cossim(i, text1)\n",
    "    if y is not None:\n",
    "        print(y)\n",
    "        similar_index.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca231d",
   "metadata": {},
   "source": [
    "how to use multipreocesing & diffference between Processor and Pool: https://urban-institute.medium.com/using-multiprocessing-to-make-python-code-faster-23ea5ef996ba\n",
    "\n",
    "Use ```multiprocess``` instead of ```multiprocessing```for ipykernel, or save the functions as .py then use it through ipykernel\n",
    "\n",
    "To append to a list between multiple processers: https://stackoverflow.com/questions/42490368/appending-to-the-same-list-from-different-processes-using-multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    with mp.Manager() as manager:\n",
    "        similar_index = manager.list() \n",
    "    \n",
    "        pool = mp.Pool(7)\n",
    "        pool.map(multiprocessing_func,range(len(documents)))\n",
    "        pool.close()\n",
    "        \n",
    "        similar_index = list(similar_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428dede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Counter(similar_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(labels = ['level_0'], axis = 1, inplace = True)\n",
    "df = df.drop(similar_index).reset_index()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4934af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdbea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da713d8a",
   "metadata": {},
   "source": [
    "### Create dataset for manual coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e936d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"tuval\",\"climate change\",\"global warming\",\"globalwarming\",\"greenhouse\",\"pollution\",\"air pollution\",\n",
    "         \"water pollution\",\"noise pollution\",\"animal protection\",\"ipcc\",\"copenhagen\",\"kyoto\",\"forest\",\n",
    "         \"two degrees\",\"carbon\",\"climate warming\",\"climatic change\",\"warming climate\",\"climatic disruption\",\n",
    "         \"climate catastrophe\",\"climate chaos\",\"climate crisis\",\"climate disaster\",\"climate emergency\",\n",
    "         \"global heating\",\"climate breakdown\",\"climate threat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cb687d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df[df['text_original'].str.contains('|'.join(terms), case=False)].groupby('outlet', group_keys=False).sample(27)\n",
    "df2 = df.drop(sampled_df.index)\n",
    "sampled_df.reset_index(inplace=True)\n",
    "os.mkdir(\"/Users/xiyan/Documents/Research/climate_compounds/Labeling/manual_benchmark\")\n",
    "sampled_df.to_csv(\"manual_benchmark/metadata_manual_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9826265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for text in sampled_df[\"text_original\"]:\n",
    "    with open(f'manual_benchmark/Text({i}).txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3cc62bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['text_original'].str.contains('|'.join(terms), case=False)].groupby('outlet', group_keys=False).sample(1)\n",
    "df2 = df2.drop(df3.index)\n",
    "df3.reset_index(inplace = True)\n",
    "os.mkdir(\"/Users/xiyan/Documents/Research/climate_compounds/Labeling/coder_training\")\n",
    "df3.to_csv(\"coder_training/metadata_coder_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8182bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for text in df3[\"text_original\"]:\n",
    "    with open(f'coder_training/Text({i}).txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3b154127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2[df2['text_original'].str.contains('|'.join(terms), case=False)].groupby('outlet', group_keys=False).sample(3)\n",
    "df4.reset_index(inplace = True)\n",
    "os.mkdir(\"/Users/xiyan/Documents/Research/climate_compounds/Labeling/coder_test\")\n",
    "df4.to_csv(\"coder_test/metadata_coder_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "742b68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for text in df4[\"text_original\"]:\n",
    "    with open(f'coder_test/Text({i}).txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9a4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83668ced",
   "metadata": {},
   "source": [
    "### Intercoder reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod1 = pd.read_excel(\"/Users/xiyan/Downloads/Coding_CoderTest3_Damiano.xlsx\")\n",
    "cod2 = pd.read_excel(\"/Users/xiyan/Downloads/Coding_CoderTest3_Pascal.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod1 = cod1.rename(columns={'RA': 'RA1'})\n",
    "cod2 = cod2.rename(columns={'RA': 'RA2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99978428",
   "metadata": {},
   "outputs": [],
   "source": [
    "coders = cod1.merge(cod2, on='ID')\n",
    "coder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coders.groupby(['RA1','RA2'])['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cada2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "krippendorff.alpha((coders['RA1'].values.tolist(), \n",
    "                    coders['RA2'].values.tolist()), level_of_measurement=\"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "coders.groupby(['RA1','RA2']).get_group((1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coders.groupby(['RA1','RA2']).get_group((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
